{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Langchain and OpenAI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "- Get setup with Langchain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x00000198444810F0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000019844483400> root_client=<openai.OpenAI object at 0x0000019841663B50> root_async_client=<openai.AsyncOpenAI object at 0x0000019844481150> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result=llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Generative AI refers to a category of artificial intelligence algorithms designed to generate new content. This can include the creation of text, images, music, and more, by learning patterns from an existing dataset. Unlike traditional AI models that might simply classify data or make predictions, generative AI models are capable of producing new data with similar characteristics to the dataset they were trained on. \\n\\nSome popular types of generative AI include:\\n\\n1. **Generative Adversarial Networks (GANs):** These consist of two neural networks, a generator and a discriminator, that are trained together. The generator creates new data samples, while the discriminator evaluates their authenticity, guiding the generator to produce better quality data over time.\\n\\n2. **Variational Autoencoders (VAEs):** These models learn the latent representations of data and then use that representation to generate new data samples that are similar to the original data.\\n\\n3. **Transformer-based Models:** Systems like GPT (Generative Pre-trained Transformer) use massive datasets to learn language patterns and generate text that's coherent and contextually relevant.\\n\\nGenerative AI has a wide range of applications, including in art, design, entertainment, education, and more, allowing for creative innovations and automation of content creation. However, it also poses challenges such as ensuring ethical use, preventing misuse in generating misleading information, and addressing intellectual property concerns.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 273, 'prompt_tokens': 13, 'total_tokens': 286, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f9f4fb6dbf', 'finish_reason': 'stop', 'logprobs': None} id='run-9b7d3109-280e-4292-9766-492d23d5b010-0' usage_metadata={'input_tokens': 13, 'output_tokens': 273, 'total_tokens': 286, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    "\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Langsmith is a platform developed by LangChain that provides tools and services for building and tuning language model applications. It offers a unified environment where developers can prototype, personalize, and deploy applications leveraging language models. Langsmith includes features for testing and debugging, as well as monitoring and evaluation to ensure the effectiveness of these applications. The platform aims to streamline the development process, making it easier for developers to create robust applications with language models. Pathways for integrating Langsmith into diverse tech stacks are also part of its offerings, supporting seamless deployment and scaling.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 33, 'total_tokens': 143, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f9f4fb6dbf', 'finish_reason': 'stop', 'logprobs': None} id='run-ba1f8f43-1fd1-428e-bc5c-65b6817eae2f-0' usage_metadata={'input_tokens': 33, 'output_tokens': 110, 'total_tokens': 143, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "## chain \n",
    "chain=prompt|llm #this means prompt is the input to llm and llm acts based on the prompt given previously to it\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Langsmith is a set of tools specifically designed to help developers working with AI applications to test, evaluate, and improve the performance of Large Language Models (LLMs). Announced in July 2023, Langsmith is a part of the LangChain ecosystem and is developed to streamline the process of debugging and enhancing LLM-driven applications, such as chatbots or text generation tools.\n",
      "\n",
      "### Features of Langsmith:\n",
      "\n",
      "1. **Logging and Tracing**: Langsmith provides capabilities to log and trace interactions within an application. This helps developers track how data flows through their application and analyze the performance of LLMs in real-time scenarios.\n",
      "\n",
      "2. **Evaluation Tools**: It offers robust evaluation tools that allow developers to set up benchmarks and compare different models or configurations. These tools are critical for understanding how changes in the model impact output quality and performance.\n",
      "\n",
      "3. **Debugging Assistance**: Langsmith includes debugging features to help identify points of failure or performance bottlenecks within an AI system. By providing detailed insights into the behavior of language models, developers can more easily resolve issues.\n",
      "\n",
      "4. **Integration with LangChain**: Being part of the LangChain ecosystem, Langsmith is designed to integrate seamlessly with other components, enabling a more coherent development experience for AI applications.\n",
      "\n",
      "5. **Real-World Application Testing**: The platform allows for real-world scenario testing, where applications can be subjected to varied and dynamic inputs to assess their robustness and reliability in practical applications.\n",
      "\n",
      "Langsmith is particularly valuable for developers seeking to enhance their LLM applications by providing tools that bring clarity and insight into model performance and application behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## stroutput Parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser #out_parser is the output parser which will parse the output of llm to string\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
